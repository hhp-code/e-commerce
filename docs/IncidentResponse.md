
## 장애대응
현재 테스트상 장애가 발생하지 않으므로, 가상 장애대응 문서를 작성합니다.

## 장애대응 문서

### 장애탐지
- **모니터링 시스템 구축**: Prometheus와 Grafana를 활용하여 실시간 시스템 모니터링 구축
- **알림 설정**: CPU 사용률, 메모리 사용량, 응답 시간 등 주요 지표에 대한 임계값 설정 및 알림 구성
-

![스크린샷 2024-08-20 13 06 34](https://github.com/user-attachments/assets/b5ad09c5-5cae-4a79-8e24-0e640c71de97)
![스크린샷 2024-08-20 13 11 50](https://github.com/user-attachments/assets/7f7f8743-e2cc-44a5-be0d-f9d983b55e2f)
### 장애공지
- **내부 공지**: Slack 채널을 통한 개발팀 및 운영팀 즉시 알림
- **외부 공지**: 사용자 대상 공지 페이지 및 푸시 알림 시스템 구축
- **공지 템플릿**: 장애 유형별 표준화된 공지 템플릿 준비

### 장애전파
- **에스컬레이션 프로세스**: 장애 심각도에 따른 단계별 보고 체계 수립
    - 1단계: 당직 엔지니어 또는 운영팀
    - 2단계: 서비스 담당 개발팀
    - 3단계: 경영진 또는 상급 조직
    - 에스컬레이션 기준: 시간기반, 영향도 기반, 복잡도 기반
- **비상 연락망**: 주요 담당자 및 의사결정권자 연락처 목록 관리
- **상황 공유 플랫폼**: Jira 또는 전용 대시보드를 통한 실시간 상황 공유

### 장애복구
- **롤백 절차**:
    - 이전 안정 버전으로의 신속한 롤백 프로세스 문서화
    - 데이터베이스 롤백 절차 및 데이터 정합성 확인 방법 정립
- **핫픽스 배포**:
    - CI/CD 파이프라인을 통한 신속한 핫픽스 배포 체계 구축
    - 코드 리뷰 및 테스트 간소화 프로세스 마련

### 장애 후속조치
- **사후 분석 회의(Post-mortem)**: 장애 원인, 대응 과정, 개선점 논의
- **재발 방지 대책**: 기술적/프로세스적 개선 사항 도출 및 이행 계획 수립
- **문서화**: 장애 보고서 작성 및 지식베이스 구축

### 장애지표 활용
- **KPI 설정**: MTTR(평균 복구 시간), MTBF(평균 장애 간격) 등 주요 지표 선정\
    - MTTR: 장애 발생부터 복구까지 걸리는 평균 시간
    - MTBF: 장애 발생 간격의 평균 시간
- **트렌드 분석**: 장애 유형, 빈도, 영향도 등에 대한 정기적인 분석 실시
- **성과 측정**: 장애 대응 프로세스 개선에 따른 효과 측정 및 보고

### 훈련 및 시뮬레이션
- **정기적인 모의 훈련**: 분기별 장애 대응 시뮬레이션 실시
- **역할 교육**: 팀원별 역할 및 책임에 대한 정기적인 교육 진행

### 커뮤니케이션 전략
- **대내외 소통 채널**: 고객 지원센터, 소셜 미디어, 이메일 등 다양한 채널 활용
- **투명성 확보**: 장애 상황 및 복구 진행 상황에 대한 투명한 공개 원칙 수립

#### 장애대응 예시

##### 장애대응 문서 예시

1. 장애 개요
- **장애 발생 일시**: 2023년 5월 15일 14:30 KST
- **장애 종료 일시**: 2023년 5월 15일 16:45 KST
- **장애 지속 시간**: 2시간 15분
- **영향 범위**: 전체 사용자의 결제 서비스 불가
- **장애 유형**: 데이터베이스 연결 오류

2. 장애 탐지
- **탐지 방법**: Grafana 모니터링 알림
- **최초 탐지 시간**: 2023년 5월 15일 14:32 KST
- **탐지 내용**: 데이터베이스 연결 시도 실패 로그 급증
- **서킷브레이커**: Reslience4J를 통한 서킷브레이커 동작 확인
- **슬로우 쿼리**: my.cnf 활성화를 통해 Slow Query Log를 통한 쿼리 성능 저하 확인

3. 장애 전파
- **내부 공유**: Slack #장애대응 채널에 즉시 공유
- **고객 공지**: 웹사이트 및 모바일 앱에 장애 안내 배너 게시

4. 대응 조치
- 14:35 - 데이터베이스 서버 상태 확인
- 14:40 - 데이터베이스 연결 풀 재설정 시도
- 15:00 - 데이터베이스 서버 재시작
- 15:30 - 백업 데이터베이스로 전환
- 16:30 - 서비스 정상화 확인

5. 원인 분석
- 주 데이터베이스 서버의 메모리 누수로 인한 연결 처리 불가
- 연결 풀 관리 로직의 버그로 인한 연결 해제 실패

6. 재발 방지 대책
- 데이터베이스 서버 메모리 모니터링 강화
- 연결 풀 관리 로직 개선 및 테스트 강화
- 자동 장애 복구 시스템 구축 계획 수립

7. 후속 조치
- 고객 대상 사과문 발송 및 보상 정책 수립
- 개발팀 대상 장애 원인 및 대응 과정 공유 세션 진행
- 운영 프로세스 개선을 위한 태스크포스 구성

8. 교훈 및 개선점
- 백업 시스템으로의 전환 시간 단축 필요
- 주요 시스템 컴포넌트에 대한 정기적인 스트레스 테스트 도입
- 장애 상황 시뮬레이션 훈련 정례화

